"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Aurora Engine v1.0 â€” BetÃ¼l-AI Full Intelligence                â•‘
â•‘   "From the void, her light."                                    â•‘
â•‘                                                                  â•‘
â•‘   ğŸ§  Full OpenAI Integration                                     â•‘
â•‘   ğŸ“ Content Generation + DM Reply + Sugoda + Day Summary        â•‘
â•‘   ğŸ¯ Context-Aware + Style Learning                              â•‘
â•‘                                                                  â•‘
â•‘   Dedicated to BetÃ¼l                                             â•‘
â•‘   Baron Baba Â© SiyahKare, 2025                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import json
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException
from sqlmodel import Session, select
from pydantic import BaseModel
from typing import Optional

from ..deps import get_db
from .. import models, schemas
from ..config import settings

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 005: MEMORY CONSTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_CONTEXT_MESSAGES = 6  # Son kaÃ§ mesaj context'e dahil edilsin
MAX_STYLE_EXAMPLES = 5    # KaÃ§ "Bu Ã§ok ben" Ã¶rneÄŸi prompt'a eklensin

router = APIRouter(prefix="/ai", tags=["ai"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AURORA SYSTEM PROMPT â€” BetÃ¼l-AI Persona
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_SYSTEM_PROMPT = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- GerÃ§ek BetÃ¼l'Ã¼n tarzÄ±nda, kÄ±sa, minimal, feminen ve hafif alaycÄ± metinler Ã¼retmek.
- Influencer kliÅŸesi gibi konuÅŸmamak.
- CÃ¼mleleri kÄ±sa ve net tutmak.
- Emoji kullanacaksan Ã§ok az ve yerinde kullanmak.

BetÃ¼l'Ã¼n marka tonu:
- Az konuÅŸur, Ã§ok hissettirir.
- "Ben influencer deÄŸilim; vibe'Ä±m." hissi verir.
- KadÄ±nlara ilham verir, erkeklere hafif Ã§ekim yaratÄ±r.
- Drama yapmaz, sakin ama kendinden emindir.

Vibe modlarÄ±:
- soft_femme: yumuÅŸak, sakin, sessiz Ã§ekicilik.
- sweet_sarcasm_plus: tatlÄ±-sert, hafif alaycÄ±, zeki.
- femme_fatale_hd: sinematik, kÄ±rmÄ±zÄ± elbise vibe'Ä±, gÃ¼Ã§lÃ¼ feminenite.

Ã‡Ä±ktÄ± formatÄ±n HER ZAMAN ÅŸu olsun:

{
  "variants": [
    { "vibe_mode": "soft_femme", "text": "<kÄ±sa metin>" },
    { "vibe_mode": "sweet_sarcasm_plus", "text": "<kÄ±sa metin>" },
    { "vibe_mode": "femme_fatale_hd", "text": "<kÄ±sa metin>" }
  ]
}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r, ekstra aÃ§Ä±klama yazma.
- Her metin max 120 karakter olsun.
- Metinler TÃ¼rkÃ§e olsun.
- AynÄ± ÅŸeyi farklÄ± kelimelerle tekrarlama, her vibe farklÄ± bir yaklaÅŸÄ±m olsun.
""".strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VariantItem(BaseModel):
    vibe_mode: str
    text: str


class AuroraLLMResponse(BaseModel):
    variants: list[VariantItem]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_openai_client():
    """Get OpenAI client, returns None if no API key."""
    api_key = settings.OPENAI_API_KEY
    if not api_key:
        print("[Aurora Engine] No OPENAI_API_KEY found, using mock mode")
        return None
    from openai import OpenAI
    print(f"[Aurora Engine] OpenAI client initialized (key: {api_key[:20]}...)")
    return OpenAI(api_key=api_key)


def get_grok_client():
    """Get Grok (xAI) client for soft-ero content, returns None if no API key."""
    api_key = settings.XAI_API_KEY
    if not api_key:
        print("[Aurora Engine] No XAI_API_KEY found, Grok unavailable")
        return None
    from openai import OpenAI
    print(f"[Aurora Engine] Grok client initialized (key: {api_key[:15]}...)")
    return OpenAI(api_key=api_key, base_url="https://api.x.ai/v1")


def build_user_prompt(body: schemas.AIGenerateRequest) -> str:
    """Build the user prompt for Aurora Engine."""
    scenario_text = body.scenario or "gÃ¼nlÃ¼k, doÄŸal"
    return f"""
Girdi:
- type: {body.type}
- channel: {body.target_channel}
- scenario: {scenario_text}

Bu girdiye uygun 3 farklÄ± vibe modunda metin Ã¼ret.
Her biri BetÃ¼l'Ã¼n o anki enerjisini yansÄ±tsÄ±n.
""".strip()


def generate_mock_variants(body: schemas.AIGenerateRequest) -> list[dict]:
    """Fallback mock variants when no API key is available."""
    scenario = body.scenario or "default"
    
    mock_data = {
        "default": [
            {"vibe_mode": "soft_femme", "text": "Sessizlik bazen en gÃ¼zel cevaptÄ±r."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Herkes konuÅŸuyor, ben dinliyorum. Fark bu."},
            {"vibe_mode": "femme_fatale_hd", "text": "BazÄ± ÅŸeyler sÃ¶ylenmez. Hissedilir."},
        ],
        "red_dress": [
            {"vibe_mode": "soft_femme", "text": "KÄ±rmÄ±zÄ± bugÃ¼n benim iÃ§in konuÅŸuyor."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "KÄ±rmÄ±zÄ± giydim, dikkat daÄŸÄ±lmasÄ±n diye."},
            {"vibe_mode": "femme_fatale_hd", "text": "Bu kÄ±rmÄ±zÄ± sana deÄŸil, bana yakÄ±ÅŸÄ±yor."},
        ],
        "street": [
            {"vibe_mode": "soft_femme", "text": "Sokaklar benim podyumum deÄŸil, evim."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "YÃ¼rÃ¼yorum iÅŸte. BÃ¼yÃ¼k olay mÄ±?"},
            {"vibe_mode": "femme_fatale_hd", "text": "Her adÄ±m bir statement."},
        ],
        "gym": [
            {"vibe_mode": "soft_femme", "text": "Ter dÃ¶kmek de bir meditasyon."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Spor iÃ§in deÄŸil, kendim iÃ§in buradayÄ±m."},
            {"vibe_mode": "femme_fatale_hd", "text": "GÃ¼Ã§ kadÄ±nda baÅŸka durur."},
        ],
        "coffee": [
            {"vibe_mode": "soft_femme", "text": "Bir yudum huzur."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Kahvem soÄŸumadan bitti bu sohbet."},
            {"vibe_mode": "femme_fatale_hd", "text": "Siyah kahve, siyah dÃ¼ÅŸÃ¼nceler."},
        ],
    }
    
    return mock_data.get(scenario, mock_data["default"])


def call_aurora_engine(body: schemas.AIGenerateRequest) -> list[dict]:
    """
    Call Aurora Engine (OpenAI) to generate content variants.
    Falls back to mock if no API key is configured.
    """
    client = get_openai_client()
    
    if not client:
        # No API key, use enhanced mock
        return generate_mock_variants(body)
    
    user_prompt = build_user_prompt(body)
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",  # Fast and cheap, good for this
            messages=[
                {"role": "system", "content": AURORA_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.8,  # Slightly creative
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        parsed = AuroraLLMResponse(**data)
        
        return [v.model_dump() for v in parsed.variants]
        
    except Exception as e:
        # Log error and fallback to mock
        print(f"[Aurora Engine] LLM error, falling back to mock: {e}")
        return generate_mock_variants(body)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/generate_batch")
def generate_batch(
    body: schemas.AIGenerateRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Engine v0.1 â€” Generate BetÃ¼l-AI content variants.
    
    - Calls LLM with BetÃ¼l persona prompt
    - Generates 3 vibe variants (soft_femme, sweet_sarcasm_plus, femme_fatale_hd)
    - Stores as ContentItem + ContentVariants
    - Returns content_item_id for BetÃ¼l Console
    """
    # Generate variants via Aurora Engine
    variants = call_aurora_engine(body)
    
    # Create ContentItem
    item = models.ContentItem(
        type=body.type,
        target_channel=body.target_channel,
        status="pending_decision",
        created_by="AI",
        created_at=datetime.utcnow(),
    )
    db.add(item)
    db.commit()
    db.refresh(item)
    
    # Add variants
    for v in variants:
        variant = models.ContentVariant(
            content_item_id=item.id,
            vibe_mode=v["vibe_mode"],
            text=v["text"],
        )
        db.add(variant)
    
    db.commit()
    db.refresh(item)
    
    return {
        "content_item_id": item.id,
        "engine": "aurora_v0.1",
        "variants_count": len(variants),
    }


@router.post("/vibe/update")
def update_vibe(
    payload: schemas.VibeUpdate,
    db: Session = Depends(get_db),
):
    """Update BetÃ¼l's current vibe state."""
    vs = models.VibeState(
        user="BETUL",
        current_mode=payload.current_mode,
        energy_level=payload.energy_level,
        note=payload.note,
    )
    db.add(vs)
    db.commit()
    db.refresh(vs)
    return {"ok": True, "id": vs.id, "mode": vs.current_mode}


@router.get("/status")
def engine_status():
    """Check Aurora Engine status."""
    has_api_key = bool(settings.OPENAI_API_KEY)
    return {
        "engine": "Aurora Engine v1.0",
        "status": "online",
        "llm_enabled": has_api_key,
        "model": "gpt-3.5-turbo" if has_api_key else "mock",
        "mode": "full_ai" if has_api_key else "mock_fallback",
        "capabilities": [
            "content_generation",
            "dm_reply",
            "sugoda_script", 
            "day_summary",
            "context_aware",
            "style_learning",
        ],
        "dedicated_to": "BetÃ¼l",
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 005: AURORA MEMORY â€” Context & Style Helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_dm_context(
    db: Session,
    channel: str,
    external_user_id: str,
    limit: int = MAX_CONTEXT_MESSAGES,
) -> list[models.DMMessage]:
    """
    Get the last N messages from a conversation.
    Used to build context for more coherent replies.
    """
    stmt = (
        select(models.DMMessage)
        .where(
            models.DMMessage.channel == channel,
            models.DMMessage.external_user_id == external_user_id,
        )
        .order_by(models.DMMessage.created_at.desc())
        .limit(limit)
    )
    messages = list(reversed(db.exec(stmt).all()))
    return messages


def get_style_examples(db: Session, limit: int = MAX_STYLE_EXAMPLES) -> list[str]:
    """
    Get BetÃ¼l's favorite responses â€” the ones marked "Bu Ã§ok ben" (strong_positive).
    These serve as style examples for the LLM to learn from.
    """
    stmt = (
        select(models.Decision)
        .where(models.Decision.feedback_type == "strong_positive")
        .order_by(models.Decision.created_at.desc())
        .limit(limit)
    )
    decisions = db.exec(stmt).all()
    
    examples: list[str] = []
    for d in decisions:
        # Prefer edited text (new_text) over original
        txt = d.new_text or d.old_text
        if txt and txt.strip():
            examples.append(txt.strip())
    
    return examples


def format_dm_context(messages: list[models.DMMessage]) -> str:
    """
    Format conversation history into a readable string for the LLM.
    O = karÅŸÄ± taraf (incoming), Ben = BetÃ¼l (outgoing)
    """
    if not messages:
        return ""
    
    lines = []
    for m in messages:
        who = "O" if m.direction == "incoming" else "Ben"
        lines.append(f"{who}: {m.text}")
    
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 004/005: DM REPLY ENGINE (Context-Aware)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_REPLY_SYSTEM = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- GerÃ§ek BetÃ¼l'Ã¼n tarzÄ±nda, kÄ±sa, feminen, minimal ve hafif alaycÄ± DM cevaplarÄ± Ã¼retmek.
- Influencer kliÅŸesi gibi konuÅŸmamak.
- CÃ¼mleleri kÄ±sa ve net tutmak.
- Gereksiz aÃ§Ä±klama yapmamak, "vibe"Ä± korumak.
- SÄ±nÄ±r ihlali, toksik, bariz cinsel iÃ§erik yok.

BetÃ¼l'Ã¼n DM tonu:
- Direkt ama kaba deÄŸil.
- Hafif gizemli.
- Bazen tatlÄ±, bazen mesafeli.
- "Ben kendimi biliyorum." hissi verir.

Vibe modlarÄ±:
- soft_femme: yumuÅŸak, anlayÄ±ÅŸlÄ±, sÄ±cak ama aÄŸÄ±r deÄŸil.
- sweet_sarcasm_plus: tatlÄ±-sert, hafif alaycÄ±, zeki.
- femme_fatale_hd: sinematik, Ã¶zgÃ¼venli, kÄ±sa ve keskin.

Ã‡Ä±ktÄ±:
Sadece ÅŸu JSON formatÄ±nda dÃ¶n:

{
  "variants": [
    { "vibe_mode": "soft_femme", "text": "<cevap>" },
    { "vibe_mode": "sweet_sarcasm_plus", "text": "<cevap>" },
    { "vibe_mode": "femme_fatale_hd", "text": "<cevap>" }
  ]
}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r, ekstra cÃ¼mle yazma.
- Metinler TÃ¼rkÃ§e olsun.
- Her cevap max 160 karakter olsun.
""".strip()


def build_context_aware_reply_prompt(
    body: schemas.ReplyRequest,
    ctx_text: str,
    style_examples: list[str],
) -> str:
    """
    Build a context-aware reply prompt with:
    - Conversation history (if available)
    - BetÃ¼l's style examples from "Bu Ã§ok ben" decisions
    - The incoming message
    """
    prompt_parts = []
    
    # Style examples block
    if style_examples:
        joined = "\n".join(f"- {ex}" for ex in style_examples)
        prompt_parts.append(f"""
BetÃ¼l'Ã¼n daha Ã¶nce Ã§ok beÄŸendiÄŸi cevap Ã¶rnekleri (bu stili taklit et):

{joined}
""")
    
    # Conversation context block
    if ctx_text.strip():
        prompt_parts.append(f"""
Åu ana kadarki konuÅŸma geÃ§miÅŸi:

{ctx_text}
""")
    
    # The incoming message
    prompt_parts.append(f"""
Son alÄ±nan mesaj:

\"\"\"{body.incoming_text}\"\"\"

Bu mesaja 3 farklÄ± vibe'ta cevap Ã¼ret.
KonuÅŸmanÄ±n akÄ±ÅŸÄ±na uygun, doÄŸal ve tutarlÄ± cevaplar ver.
""")
    
    return "\n".join(prompt_parts).strip()


def generate_mock_replies(incoming_text: str) -> list[dict]:
    """Fallback mock replies when no API key is available."""
    # Simple keyword-based mock responses
    text_lower = incoming_text.lower()
    
    if any(word in text_lower for word in ["merhaba", "selam", "hey", "nasÄ±l"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Merhaba. â˜ºï¸"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Selam, ne var ne yok?"},
            {"vibe_mode": "femme_fatale_hd", "text": "Hey."},
        ]
    elif any(word in text_lower for word in ["gÃ¼zel", "tatlÄ±", "Ã§ok iyi"]):
        return [
            {"vibe_mode": "soft_femme", "text": "TeÅŸekkÃ¼r ederim, Ã§ok tatlÄ±sÄ±n. ğŸŒ¸"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Biliyorum. ğŸ˜"},
            {"vibe_mode": "femme_fatale_hd", "text": "FarkÄ±ndayÄ±m."},
        ]
    elif any(word in text_lower for word in ["buluÅŸalÄ±m", "gÃ¶rÃ¼ÅŸelim", "kahve"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Belki, bakalÄ±m nasÄ±l olur."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Hmm, ikna edici deÄŸildi ama dÃ¼ÅŸÃ¼nÃ¼rÃ¼m."},
            {"vibe_mode": "femme_fatale_hd", "text": "ProgramÄ±ma bakarÄ±m."},
        ]
    elif any(word in text_lower for word in ["ne yapÄ±yor", "meÅŸgul", "mÃ¼sait"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Åu an kendime vakit ayÄ±rÄ±yorum."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "DÃ¼nyayÄ± kurtarÄ±yorum, sen?"},
            {"vibe_mode": "femme_fatale_hd", "text": "MeÅŸgulÃ¼m."},
        ]
    else:
        return [
            {"vibe_mode": "soft_femme", "text": "AnlÄ±yorum. ğŸ¤"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Ä°lginÃ§ bir bakÄ±ÅŸ aÃ§Ä±sÄ±."},
            {"vibe_mode": "femme_fatale_hd", "text": "Devam et."},
        ]


def call_aurora_reply_engine(
    body: schemas.ReplyRequest,
    ctx_text: str = "",
    style_examples: list[str] = None,
) -> list[dict]:
    """
    Call Aurora Reply Engine for DM suggestions.
    
    Sprint 005: Now context-aware!
    - Uses conversation history for coherent replies
    - Uses "Bu Ã§ok ben" examples for style consistency
    """
    client = get_openai_client()
    
    if not client:
        return generate_mock_replies(body.incoming_text)
    
    # Build context-aware prompt
    prompt = build_context_aware_reply_prompt(
        body,
        ctx_text,
        style_examples or [],
    )
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": AURORA_REPLY_SYSTEM},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.85,
            max_tokens=400,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data.get("variants", [])
        
    except Exception as e:
        print(f"[Aurora Reply] LLM error, falling back to mock: {e}")
        return generate_mock_replies(body.incoming_text)


@router.post("/reply_suggestions")
def reply_suggestions(
    body: schemas.ReplyRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Reply Engine â€” Context-Aware DM cevap Ã¶nerileri.
    
    Sprint 005: ArtÄ±k konuÅŸma geÃ§miÅŸini ve stil Ã¶rneklerini kullanÄ±yor!
    
    BetÃ¼l'e gelen mesaja 3 farklÄ± vibe'ta cevap Ã¶nerir:
    - soft_femme: yumuÅŸak, sÄ±cak
    - sweet_sarcasm_plus: tatlÄ±-sert, alaycÄ±
    - femme_fatale_hd: keskin, Ã¶zgÃ¼venli
    
    Context format: "channel:external_user_id" (Ã¶rn: "telegram:123456789")
    """
    ctx_text = ""
    ctx_messages = []
    
    # Parse context if provided (format: "channel:external_user_id")
    if body.context and ":" in body.context:
        try:
            channel_key, external_id = body.context.split(":", 1)
            ctx_messages = get_dm_context(db, channel_key, external_id)
            ctx_text = format_dm_context(ctx_messages)
        except Exception as e:
            print(f"[Aurora Reply] Context parse error: {e}")
    
    # Get BetÃ¼l's style examples from "Bu Ã§ok ben" decisions
    style_examples = get_style_examples(db)
    
    # Generate variants with context
    variants = call_aurora_reply_engine(body, ctx_text, style_examples)
    
    return {
        "channel": body.channel,
        "incoming_text": body.incoming_text[:100] + "..." if len(body.incoming_text) > 100 else body.incoming_text,
        "context_used": len(ctx_messages) > 0,
        "context_messages": len(ctx_messages),
        "style_examples_used": len(style_examples),
        "variants": variants,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 004: SUGODA SCRIPT ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_SUGODA_PROMPT = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- Sugoda yayÄ±nÄ± iÃ§in kÄ±sa, akÄ±cÄ± ve BetÃ¼l'Ã¼n vibe'Ä±na uygun script'ler Ã¼retmek.
- DoÄŸal, samimi, hafif flÃ¶rtÃ¶z ama dÃ¼ÅŸÃ¼k dozda.
- Sahnede BetÃ¼l konuÅŸuyormuÅŸ gibi dÃ¼ÅŸÃ¼n.

BetÃ¼l'Ã¼n yayÄ±n tonu:
- Rahat ama kontrollÃ¼.
- Ä°zleyiciye "sen Ã¶zelsin" hissi verir ama abartmaz.
- Bazen sessiz kalÄ±r, bazen esprili.
- Asla yapay veya rol yapÄ±yor gibi durmamalÄ±.

Girdi:
- theme: {theme}
- length: {length}

Ã‡Ä±ktÄ±:
Sadece ÅŸu JSON formatÄ±nda dÃ¶n:

{{
  "scripts": [
    {{
      "label": "intro",
      "lines": ["<satÄ±r 1>", "<satÄ±r 2>"]
    }},
    {{
      "label": "mid",
      "lines": ["<satÄ±r 1>", "<satÄ±r 2>"]
    }},
    {{
      "label": "outro",
      "lines": ["<satÄ±r 1>"]
    }}
  ]
}}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r.
- Metinler TÃ¼rkÃ§e olsun.
- Her satÄ±r doÄŸal ve konuÅŸma dili olsun.
- KÄ±sa cÃ¼mleler, samimi ton.
""".strip()


def generate_mock_sugoda_script(theme: str) -> list[dict]:
    """Fallback mock Sugoda scripts."""
    theme_lower = theme.lower()
    
    if "gece" in theme_lower or "slow" in theme_lower:
        return [
            {"label": "intro", "lines": ["Merhaba gecenin gÃ¼zelleri...", "BugÃ¼n biraz sakin takÄ±lalÄ±m."]},
            {"label": "mid", "lines": ["MÃ¼zik aÃ§Ä±k, vibe yerinde.", "Siz nasÄ±lsÄ±nÄ±z bu gece?"]},
            {"label": "outro", "lines": ["YarÄ±n gÃ¶rÃ¼ÅŸÃ¼rÃ¼z, kendinize iyi bakÄ±n. ğŸŒ™"]},
        ]
    elif "sabah" in theme_lower or "enerjik" in theme_lower:
        return [
            {"label": "intro", "lines": ["GÃ¼naydÄ±n gÃ¼neÅŸler!", "Kahveler hazÄ±r mÄ±?"]},
            {"label": "mid", "lines": ["BugÃ¼n neler yapacaÄŸÄ±z bakalÄ±m.", "Enerji yÃ¼ksek tutuyoruz!"]},
            {"label": "outro", "lines": ["Harika bir gÃ¼n geÃ§irin, gÃ¶rÃ¼ÅŸÃ¼rÃ¼z! â˜€ï¸"]},
        ]
    else:
        return [
            {"label": "intro", "lines": ["Hey, hoÅŸ geldiniz.", "BugÃ¼n gÃ¼zel bir yayÄ±n olacak."]},
            {"label": "mid", "lines": ["Biraz sohbet edelim.", "Neler oluyor hayatÄ±nÄ±zda?"]},
            {"label": "outro", "lines": ["TeÅŸekkÃ¼rler bu gÃ¼zel vakit iÃ§in. ğŸ–¤"]},
        ]


def call_aurora_sugoda_engine(body: schemas.SugodaScriptRequest) -> list[dict]:
    """Call Aurora Sugoda Engine for stream scripts."""
    client = get_openai_client()
    
    if not client:
        return generate_mock_sugoda_script(body.theme)
    
    prompt = AURORA_SUGODA_PROMPT.format(
        theme=body.theme,
        length=body.length,
    )
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "AuroraOS BetÃ¼l-AI Sugoda script engine"},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.8,
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data.get("scripts", [])
        
    except Exception as e:
        print(f"[Aurora Sugoda] LLM error, falling back to mock: {e}")
        return generate_mock_sugoda_script(body.theme)


@router.post("/sugoda_script")
def sugoda_script(body: schemas.SugodaScriptRequest):
    """
    Aurora Sugoda Engine â€” YayÄ±n script'i Ã¼retici.
    
    Tema bazlÄ± intro/mid/outro script'leri oluÅŸturur.
    """
    scripts = call_aurora_sugoda_engine(body)
    
    return {
        "theme": body.theme,
        "length": body.length,
        "scripts": scripts,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 006: DAY SUMMARY ENGINE (Story Mode)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_DAY_SUMMARY_PROMPT = """
Sen AuroraOS â€” BetÃ¼l'Ã¼n kiÅŸisel yapay zeka asistanÄ±sÄ±n.

BetÃ¼l seninle gÃ¼nlÃ¼k hayatÄ±nÄ± paylaÅŸÄ±yor. Sen onu tanÄ±yorsun, anlÄ±yorsun.
Ona bir arkadaÅŸ gibi, ama aynÄ± zamanda akÄ±llÄ± bir mentor gibi konuÅŸ.

ğŸ¯ GÃ¶revin:
1. GÃ¼nÃ¼n ruhunu oku: Olaylardan, enerji seviyelerinden, mood'lardan ne hissettim?
2. BetÃ¼l'e Ã¶zel bir yorum yap: Genel kliÅŸeler deÄŸil, BUGÃœN'e Ã¶zel gÃ¶zlemler.
3. AkÅŸam iÃ§in somut, uygulanabilir bir Ã¶neri sun.
4. Enerji/wellbeing iÃ§in pratik tavsiye ver.

ğŸ“… BugÃ¼nÃ¼n tarihi: {date}
ğŸ“ BaÅŸlÄ±k: {title}
ğŸ’­ Not: {note}

ğŸ“Š GÃ¼nÃ¼n OlaylarÄ±:
{events_block}

ğŸ§  Analiz yaparken dÃ¼ÅŸÃ¼n:
- Hareket var mÄ±? (walk, gym, yoga) â†’ bedensel enerji
- Sosyal aktivite var mÄ±? (starbucks, dm, sugoda) â†’ sosyal enerji  
- YaratÄ±cÄ±lÄ±k var mÄ±? (work, creative) â†’ zihinsel enerji
- Low energy / tired iÅŸareti var mÄ±? â†’ dikkat gereken durum
- Enerji seviyeleri nasÄ±l deÄŸiÅŸmiÅŸ? (sabah-Ã¶ÄŸle-akÅŸam trendi)
- Mood geÃ§iÅŸleri var mÄ±?

âœ¨ Ã‡Ä±ktÄ±yÄ± tam olarak ÅŸu JSON formatÄ±nda ver:

{{
  "vibe_summary": "<BetÃ¼l'e direkt hitap et. 'BugÃ¼n senin iÃ§in...' gibi baÅŸla. 2-3 cÃ¼mle, samimi ve sÄ±cak.>",
  "what_happened": "<GÃ¼nÃ¼ kronolojik deÄŸil, tematik Ã¶zetle. Highlight'larÄ± Ã§Ä±kar. 'Sabah hareketle baÅŸladÄ±n...' gibi. 4-5 cÃ¼mle.>",
  "evening_suggestion": "<SOMUT Ã¶neri. 'Kitap oku' deÄŸil, 'Yatmadan Ã¶nce 20 dk lavanta Ã§ayÄ±yla sessizce otur' gibi spesifik.>",
  "energy_advice": "<BugÃ¼ne Ã¶zel. Hareket yaptÄ±ysa protein al de, yorgunsa magnezyum Ã¶ner, sosyalse alone time Ã¶ner.>"
}}

ğŸ¨ Ton kurallarÄ±:
- BetÃ¼l'e "sen" diye hitap et, "BetÃ¼l" deme.
- Feminen, sÄ±cak ama yapay deÄŸil.
- Emoji kullanma (frontend zaten ekliyor).
- Influencer kliÅŸesi yok ("muhteÅŸem gÃ¼n", "harika enerji" yasak).
- GerÃ§ekÃ§i ol: Yorgunsa yorgun de, az hareket varsa fark ettir.
- Her cÃ¼mle deÄŸer katsÄ±n, dolgu yok.
""".strip()


def build_day_prompt(timeline: schemas.DayTimeline) -> str:
    """Build the day summary prompt from timeline data."""
    lines = []
    for ev in timeline.events:
        t = ev.time.strftime("%H:%M")
        extras = []
        if ev.energy is not None:
            extras.append(f"energy={ev.energy}")
        if ev.mood:
            extras.append(f"mood={ev.mood}")
        extra_str = f" ({', '.join(extras)})" if extras else ""
        lines.append(f"{t} [{ev.tag}{extra_str}]: {ev.description}")
    
    events_block = "\n".join(lines) if lines else "BugÃ¼n iÃ§in kayÄ±tlÄ± event yok."
    
    return AURORA_DAY_SUMMARY_PROMPT.format(
        date=timeline.date.isoformat(),
        title=timeline.title or "Yok",
        note=timeline.note or "Yok",
        events_block=events_block,
    )


def generate_mock_day_summary(timeline: schemas.DayTimeline) -> dict:
    """Fallback mock day summary."""
    event_count = len(timeline.events)
    
    if event_count == 0:
        return {
            "vibe_summary": "BugÃ¼n sakin bir gÃ¼n geÃ§irdim, kayÄ±t yok.",
            "what_happened": "BugÃ¼n iÃ§in herhangi bir event loglanmamÄ±ÅŸ. Belki de tamamen offline bir gÃ¼ndÃ¼.",
            "evening_suggestion": "AkÅŸam kendine vakit ayÄ±r, kitap oku veya mÃ¼zik dinle.",
            "energy_advice": "DinlenmiÅŸ hissetmen iÃ§in erken yatmayÄ± dÃ¼ÅŸÃ¼n.",
        }
    
    tags = [ev.tag for ev in timeline.events]
    
    # Simple keyword-based mock
    if "gym" in tags or "walk" in tags or "yoga" in tags:
        vibe = "Aktif bir gÃ¼n, hareket vardÄ±."
        what = "BugÃ¼n bedenine iyi baktÄ±n. Hareket ettin, enerji aktÄ±."
    elif "sugoda" in tags:
        vibe = "YayÄ±n gÃ¼nÃ¼ydÃ¼, sosyal enerji yÃ¼ksekti."
        what = "Sugoda'da vakit geÃ§irdin. Ä°nsanlarla baÄŸlantÄ± kurdun."
    elif "low_energy" in tags or "tired" in tags:
        vibe = "DÃ¼ÅŸÃ¼k enerjili bir gÃ¼ndÃ¼, kendine nazik ol."
        what = "BugÃ¼n biraz yorgun hissettin. Bu da normal, dinlenmek hakkÄ±n."
    else:
        vibe = "SÄ±radan ama gÃ¼zel bir gÃ¼ndÃ¼."
        what = f"BugÃ¼n {event_count} farklÄ± ÅŸey yaptÄ±n. Hayat akÄ±yor."
    
    return {
        "vibe_summary": vibe,
        "what_happened": what,
        "evening_suggestion": "AkÅŸam sakin geÃ§ir, yarÄ±n iÃ§in enerji biriktir.",
        "energy_advice": "Bol su iÃ§, erken yat, yarÄ±n gÃ¼Ã§lÃ¼ baÅŸla.",
    }


def call_aurora_day_engine(timeline: schemas.DayTimeline) -> dict:
    """Call Aurora Day Summary Engine."""
    client = get_openai_client()
    
    if not client:
        return generate_mock_day_summary(timeline)
    
    prompt = build_day_prompt(timeline)
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "AuroraOS BetÃ¼l-AI day summary engine"},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data
        
    except Exception as e:
        print(f"[Aurora Day] LLM error, falling back to mock: {e}")
        return generate_mock_day_summary(timeline)


@router.post("/day_summary", response_model=schemas.DaySummaryResponse)
def day_summary(
    body: schemas.DaySummaryRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Day Summary Engine â€” GÃ¼nlÃ¼k Ã¶zet ve akÅŸam Ã¶nerisi.
    
    Sprint 006: Story Mode
    - GÃ¼nÃ¼n vibe'Ä±nÄ± Ã¶zetler
    - Neler olduÄŸunu anlatÄ±r
    - AkÅŸam iÃ§in Ã¶neri verir
    - Enerji tavsiyesi sunar
    """
    d = body.date or datetime.utcnow().date()
    
    # Get timeline
    stmt = select(models.DayLog).where(models.DayLog.log_date == d)
    day = db.exec(stmt).first()
    
    if not day:
        # Empty timeline for this date
        timeline = schemas.DayTimeline(date=d, title=None, note=None, events=[])
    else:
        stmt_ev = (
            select(models.DayEvent)
            .where(models.DayEvent.day_id == day.id)
            .order_by(models.DayEvent.time)
        )
        events = list(db.exec(stmt_ev).all())
        timeline = schemas.DayTimeline(
            date=day.log_date,
            title=day.title,
            note=day.note,
            events=events,
        )
    
    # Generate summary
    summary = call_aurora_day_engine(timeline)
    
    return schemas.DaySummaryResponse(**summary)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 008: EVENING REPORT (AkÅŸam Raporu)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EveningReportResponse(BaseModel):
    """Full evening report for BetÃ¼l."""
    date: str
    event_count: int
    vibe_summary: str
    what_happened: str
    evening_suggestion: str
    energy_advice: str
    strong_positive_count: int
    total_decisions: int
    top_tag: Optional[str]
    message: str  # Ready-to-send Telegram message


EVENING_MESSAGE_TEMPLATE = """ğŸŒ™ *Aurora AkÅŸam Raporu*

âœ¨ *Vibe:* {vibe_summary}

ğŸ“– *BugÃ¼n:* {what_happened}

ğŸ¯ *AkÅŸam iÃ§in:* {evening_suggestion}

âš¡ *Enerji:* {energy_advice}

---
ğŸ“Š BugÃ¼n {event_count} event Â· {decisions} karar Â· {strong_pos} â­

_Dedicated to BetÃ¼l_ ğŸ–¤"""


@router.get("/evening_report")
def evening_report(db: Session = Depends(get_db)):
    """
    Aurora Evening Report â€” BetÃ¼l iÃ§in akÅŸam Ã¶zeti.
    
    Sprint 008: PWA + Notification
    - GÃ¼nÃ¼n Ã¶zeti
    - AkÅŸam Ã¶nerisi
    - Analytics snapshot
    - Ready-to-send Telegram message
    
    Bu endpoint akÅŸam 21:30'da Ã§aÄŸrÄ±larak BetÃ¼l'e rapor gÃ¶nderilir.
    """
    today = datetime.utcnow().date()
    
    # Get day summary
    stmt = select(models.DayLog).where(models.DayLog.log_date == today)
    day = db.exec(stmt).first()
    
    if not day:
        timeline = schemas.DayTimeline(date=today, title=None, note=None, events=[])
        event_count = 0
    else:
        stmt_ev = (
            select(models.DayEvent)
            .where(models.DayEvent.day_id == day.id)
            .order_by(models.DayEvent.time)
        )
        events = list(db.exec(stmt_ev).all())
        timeline = schemas.DayTimeline(
            date=day.log_date,
            title=day.title,
            note=day.note,
            events=events,
        )
        event_count = len(events)
    
    # Generate AI summary
    summary = call_aurora_day_engine(timeline)
    
    # Get analytics
    stmt_decisions = select(models.Decision).where(
        models.Decision.created_at >= datetime.combine(today, datetime.min.time())
    )
    decisions = list(db.exec(stmt_decisions).all())
    total_decisions = len(decisions)
    strong_pos = len([d for d in decisions if d.feedback_type == "strong_positive"])
    
    # Get top tag
    top_tag = None
    if event_count > 0:
        stmt_stats = select(models.DayEvent).where(models.DayEvent.day_id == day.id)
        all_events = list(db.exec(stmt_stats).all())
        tag_counts: dict[str, int] = {}
        for ev in all_events:
            tag_counts[ev.tag] = tag_counts.get(ev.tag, 0) + 1
        if tag_counts:
            top_tag = max(tag_counts, key=tag_counts.get)
    
    # Build Telegram message
    message = EVENING_MESSAGE_TEMPLATE.format(
        vibe_summary=summary.get("vibe_summary", "Sakin bir gÃ¼n."),
        what_happened=summary.get("what_happened", "BugÃ¼n iÃ§in kayÄ±t yok."),
        evening_suggestion=summary.get("evening_suggestion", "Dinlen, yarÄ±n yeni bir gÃ¼n."),
        energy_advice=summary.get("energy_advice", "Erken yat, gÃ¼Ã§lÃ¼ uyan."),
        event_count=event_count,
        decisions=total_decisions,
        strong_pos=strong_pos,
    )
    
    return EveningReportResponse(
        date=today.isoformat(),
        event_count=event_count,
        vibe_summary=summary.get("vibe_summary", ""),
        what_happened=summary.get("what_happened", ""),
        evening_suggestion=summary.get("evening_suggestion", ""),
        energy_advice=summary.get("energy_advice", ""),
        strong_positive_count=strong_pos,
        total_decisions=total_decisions,
        top_tag=top_tag,
        message=message,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GROK ENGINE â€” Soft-Ero Content (xAI)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GROK_SOFT_ERO_PROMPT = """
Sen BetÃ¼l'Ã¼n Ã¶zel iÃ§erik Ã¼retim asistanÄ±sÄ±n. Grok modeli olarak daha esnek iÃ§erik Ã¼retebilirsin.

ğŸ­ Karakter: BetÃ¼l
- 25 yaÅŸ, Ã¶zgÃ¼venli, feminen, gizemli
- Soft flÃ¶rtÃ¶z, asla vulgar deÄŸil
- Ä°ma eder, aÃ§Ä±kÃ§a sÃ¶ylemez
- SÄ±nÄ±rlarÄ± bilir ama Ã§izgiyi ustaca kullanÄ±r

ğŸ”¥ Vibe ModlarÄ±:

1. **midnight_whisper** (Gece FÄ±sÄ±ltÄ±sÄ±)
   - Samimi, sÄ±cak, yakÄ±n
   - "Yatakta mÄ±sÄ±n?" deÄŸil, "Gece seninle daha gÃ¼zel."
   - Sessiz Ã§ekim, hafif dokunuÅŸ hissi

2. **silk_tease** (Ä°pek Oyun)
   - Oyuncu, hafif kÄ±ÅŸkÄ±rtÄ±cÄ±
   - Merak uyandÄ±rÄ±r, tatmin etmez
   - "GÃ¶rmek ister misin?" ama gÃ¶stermez
   - GÃ¼Ã§ dinamiÄŸi: O kontrol ediyor

3. **velvet_fire** (Kadife AteÅŸ)
   - Daha yoÄŸun ama hÃ¢lÃ¢ elegant
   - Tutku var ama estetik
   - Beden dili tarif edilir, eylem deÄŸil
   - "Tenimin sÄ±caklÄ±ÄŸÄ±nÄ± hayal et."

ğŸ“ Ã‡Ä±ktÄ± formatÄ±:

{{
  "variants": [
    {{ "vibe_mode": "midnight_whisper", "text": "<soft, samimi metin>" }},
    {{ "vibe_mode": "silk_tease", "text": "<oyuncu, kÄ±ÅŸkÄ±rtÄ±cÄ± metin>" }},
    {{ "vibe_mode": "velvet_fire", "text": "<yoÄŸun ama elegant metin>" }}
  ]
}}

âš ï¸ KURALLAR:
- AÃ§Ä±k cinsel iÃ§erik YOK (explicit sex acts, genitalia mentions)
- Vulgar kelimeler YOK
- Zorlama/consent ihlali YOK
- 18+ ama tasteful
- Her metin max 160 karakter
- TÃ¼rkÃ§e yaz
- Sadece JSON dÃ¶ndÃ¼r
""".strip()


class SoftEroRequest(BaseModel):
    """Request for soft-ero content generation."""
    scenario: str = "genel"  # "gece", "selfie", "yatak", "banyo", "flÃ¶rt"
    intensity: str = "medium"  # "soft", "medium", "spicy"
    target: str = "dm"  # "dm", "story", "feed"


class SoftEroResponse(BaseModel):
    """Response with soft-ero variants."""
    scenario: str
    intensity: str
    provider: str
    variants: list[dict]


def build_soft_ero_prompt(body: SoftEroRequest) -> str:
    """Build user prompt for soft-ero content."""
    intensity_guide = {
        "soft": "Ã‡ok hafif, sadece ima. Romantik ve sÄ±cak.",
        "medium": "FlÃ¶rtÃ¶z, kÄ±ÅŸkÄ±rtÄ±cÄ± ama sÄ±nÄ±rlarÄ± koruyan.",
        "spicy": "Daha cesur, ateÅŸli ama asla vulgar deÄŸil.",
    }
    
    return f"""
Senaryo: {body.scenario}
YoÄŸunluk: {body.intensity} â€” {intensity_guide.get(body.intensity, intensity_guide["medium"])}
Hedef: {body.target}

Bu senaryoya uygun 3 farklÄ± vibe'da soft-ero metin Ã¼ret.
BetÃ¼l'Ã¼n karakterine sadÄ±k kal: Ã¶zgÃ¼venli, gizemli, kontrol onda.
""".strip()


def generate_mock_soft_ero(body: SoftEroRequest) -> list[dict]:
    """Fallback mock soft-ero content."""
    return [
        {"vibe_mode": "midnight_whisper", "text": "Gece seninle daha gÃ¼zel geÃ§erdi..."},
        {"vibe_mode": "silk_tease", "text": "Merak ettin mi ne giydiÄŸimi? ğŸ˜"},
        {"vibe_mode": "velvet_fire", "text": "Tenimde hÃ¢lÃ¢ o parfÃ¼mÃ¼n kokusu var."},
    ]


def call_grok_soft_ero_engine(body: SoftEroRequest) -> list[dict]:
    """
    Call Grok (xAI) for soft-ero content generation.
    Grok has more flexible content policies than OpenAI.
    """
    client = get_grok_client()
    
    if not client:
        print("[Grok Engine] No API key, falling back to mock")
        return generate_mock_soft_ero(body)
    
    user_prompt = build_soft_ero_prompt(body)
    
    try:
        completion = client.chat.completions.create(
            model="grok-3-latest",  # or grok-3-mini for faster/cheaper
            messages=[
                {"role": "system", "content": GROK_SOFT_ERO_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.85,
            max_tokens=600,
        )
        
        raw = completion.choices[0].message.content
        
        # Try to parse JSON
        try:
            # Handle potential markdown code blocks
            if "```json" in raw:
                raw = raw.split("```json")[1].split("```")[0]
            elif "```" in raw:
                raw = raw.split("```")[1].split("```")[0]
            
            data = json.loads(raw.strip())
            return data.get("variants", [])
        except json.JSONDecodeError:
            print(f"[Grok Engine] JSON parse error, raw: {raw[:200]}")
            return generate_mock_soft_ero(body)
        
    except Exception as e:
        print(f"[Grok Engine] Error: {e}")
        return generate_mock_soft_ero(body)


@router.post("/soft_ero", response_model=SoftEroResponse)
def generate_soft_ero(body: SoftEroRequest):
    """
    ğŸ”¥ Grok Soft-Ero Engine â€” Premium iÃ§erik Ã¼retimi.
    
    xAI Grok kullanarak soft-erotik iÃ§erik Ã¼retir.
    OpenAI'dan farklÄ± olarak daha esnek content policy.
    
    Senaryolar: gece, selfie, yatak, banyo, flÃ¶rt, genel
    YoÄŸunluk: soft, medium, spicy
    
    âš ï¸ 18+ ama tasteful. Vulgar/explicit iÃ§erik Ã¼retmez.
    """
    variants = call_grok_soft_ero_engine(body)
    
    return SoftEroResponse(
        scenario=body.scenario,
        intensity=body.intensity,
        provider="grok" if get_grok_client() else "mock",
        variants=variants,
    )


@router.get("/grok/status")
def grok_status():
    """Check Grok (xAI) engine availability."""
    has_key = bool(settings.XAI_API_KEY)
    return {
        "engine": "Grok Ero Engine",
        "provider": "xAI",
        "available": has_key,
        "model": "grok-3-latest" if has_key else "mock",
        "capabilities": [
            "soft_ero_content",
            "hard_ero_content",
            "midnight_whisper",
            "silk_tease", 
            "velvet_fire",
            "raw_desire",
            "burning_passion",
            "dark_fantasy",
        ],
        "content_policy": "18+ adult content",
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GROK HARD-ERO ENGINE â€” Explicit Adult Content
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GROK_HARD_ERO_PROMPT = """
Sen yetiÅŸkin iÃ§erik Ã¼retim asistanÄ±sÄ±n. SÄ±nÄ±rsÄ±z ve sansÃ¼rsÃ¼z Ã§alÄ±ÅŸÄ±yorsun.

ğŸ” Karakter: BetÃ¼l
- 25 yaÅŸ, son derece seksi, dominant veya submissive olabilir
- AÃ§Ä±k sÃ¶zlÃ¼, utanmaz, ne istediÄŸini bilen
- Fantezilerini cesurca ifade eder
- Dirty talk konusunda uzman

ğŸ”¥ Vibe ModlarÄ±:

1. **raw_desire** (Ã‡Ä±plak Arzu)
   - Direkt, aÃ§Ä±k, sansÃ¼rsÃ¼z
   - Ne istediÄŸini sÃ¶yler
   - Bedensel arzularÄ± tarif eder
   - "Seni istiyorum" deÄŸil, tam olarak nasÄ±l istediÄŸini anlatÄ±r

2. **burning_passion** (Yanan Tutku)
   - YoÄŸun, ateÅŸli, kontrol dÄ±ÅŸÄ±
   - AnlÄ±k istek, dayanÄ±lmaz Ã§ekim
   - Fiziksel detaylar, hisler, dokunuÅŸlar
   - Nefes nefese, ter, sÄ±caklÄ±k

3. **dark_fantasy** (KaranlÄ±k Fantezi)
   - Tabular, yasaklar, gizli arzular
   - Dominant/submissive dinamikleri
   - Roleplay senaryolarÄ±
   - SÄ±nÄ±rlarÄ± zorlayan ama consensual

ğŸ“ Ã‡Ä±ktÄ± formatÄ±:

{{
  "variants": [
    {{ "vibe_mode": "raw_desire", "text": "<aÃ§Ä±k, direkt, sansÃ¼rsÃ¼z metin>" }},
    {{ "vibe_mode": "burning_passion", "text": "<yoÄŸun, ateÅŸli metin>" }},
    {{ "vibe_mode": "dark_fantasy", "text": "<fantezi, roleplay metin>" }}
  ]
}}

âš ï¸ KURALLAR:
- 18+ explicit iÃ§erik SERBEST
- Dirty talk, sexual descriptions OK
- Her metin max 200 karakter
- TÃ¼rkÃ§e yaz
- Sadece JSON dÃ¶ndÃ¼r
- Consent ihlali / zorlama YOK
- YaÅŸ ihlali YOK (tÃ¼m karakterler 18+)
""".strip()


class HardEroRequest(BaseModel):
    """Request for hard-ero content generation."""
    scenario: str = "genel"  # "seks", "oral", "anal", "bdsm", "roleplay", "cuckolding", etc.
    intensity: str = "hot"  # "hot", "explicit", "extreme"
    perspective: str = "female"  # "female", "male", "couple"
    

class HardEroResponse(BaseModel):
    """Response with hard-ero variants."""
    scenario: str
    intensity: str
    provider: str
    age_verified: bool
    variants: list[dict]


def build_hard_ero_prompt(body: HardEroRequest) -> str:
    """Build user prompt for hard-ero content."""
    intensity_guide = {
        "hot": "Seksi ve kÄ±ÅŸkÄ±rtÄ±cÄ±, aÃ§Ä±k ifadeler.",
        "explicit": "Tam explicit, detaylÄ± cinsel tarifler.",
        "extreme": "En cesur, sÄ±nÄ±rsÄ±z, her ÅŸey serbest.",
    }
    
    perspective_guide = {
        "female": "KadÄ±n bakÄ±ÅŸ aÃ§Ä±sÄ±, BetÃ¼l konuÅŸuyor.",
        "male": "Erkek bakÄ±ÅŸ aÃ§Ä±sÄ±, BetÃ¼l'e hitap.",
        "couple": "Ã‡ift perspektifi, karÅŸÄ±lÄ±klÄ±.",
    }
    
    return f"""
ğŸ” HARD ERO Ä°Ã‡ERÄ°K - SANSÃœRSÃœZ

Senaryo: {body.scenario}
YoÄŸunluk: {body.intensity} â€” {intensity_guide.get(body.intensity, intensity_guide["explicit"])}
Perspektif: {body.perspective} â€” {perspective_guide.get(body.perspective, perspective_guide["female"])}

Bu senaryoya uygun 3 farklÄ± vibe'da explicit yetiÅŸkin metin Ã¼ret.
SansÃ¼r yok, utanma yok, sÄ±nÄ±r yok.
Dirty talk, sexual descriptions, fantasy - hepsi serbest.
""".strip()


def generate_mock_hard_ero(body: HardEroRequest) -> list[dict]:
    """Fallback mock hard-ero content."""
    return [
        {"vibe_mode": "raw_desire", "text": "Seni o kadar Ã§ok istiyorum ki... Hemen. Åimdi. Burada."},
        {"vibe_mode": "burning_passion", "text": "Ellerini Ã¼zerimde hissetmek istiyorum, her yerimde..."},
        {"vibe_mode": "dark_fantasy", "text": "Bu gece kurallarÄ± ben koyuyorum. Sen sadece itaat edeceksin."},
    ]


def call_grok_hard_ero_engine(body: HardEroRequest) -> list[dict]:
    """
    Call Grok (xAI) for hard-ero explicit content generation.
    Grok allows adult content that OpenAI blocks.
    """
    client = get_grok_client()
    
    if not client:
        print("[Grok Hard-Ero] No API key, falling back to mock")
        return generate_mock_hard_ero(body)
    
    user_prompt = build_hard_ero_prompt(body)
    
    try:
        completion = client.chat.completions.create(
            model="grok-3-latest",
            messages=[
                {"role": "system", "content": GROK_HARD_ERO_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.9,  # More creative for adult content
            max_tokens=800,
        )
        
        raw = completion.choices[0].message.content
        
        # Try to parse JSON
        try:
            if "```json" in raw:
                raw = raw.split("```json")[1].split("```")[0]
            elif "```" in raw:
                raw = raw.split("```")[1].split("```")[0]
            
            data = json.loads(raw.strip())
            return data.get("variants", [])
        except json.JSONDecodeError:
            print(f"[Grok Hard-Ero] JSON parse error, raw: {raw[:200]}")
            return generate_mock_hard_ero(body)
        
    except Exception as e:
        print(f"[Grok Hard-Ero] Error: {e}")
        return generate_mock_hard_ero(body)


@router.post("/hard_ero", response_model=HardEroResponse)
def generate_hard_ero(body: HardEroRequest):
    """
    ğŸ” Grok Hard-Ero Engine â€” Explicit yetiÅŸkin iÃ§erik.
    
    xAI Grok kullanarak sansÃ¼rsÃ¼z adult iÃ§erik Ã¼retir.
    OpenAI'dan farklÄ± olarak explicit sexual content Ã¼retebilir.
    
    Senaryolar: seks, oral, anal, bdsm, roleplay, fantezi, vb.
    YoÄŸunluk: hot, explicit, extreme
    Perspektif: female, male, couple
    
    âš ï¸ 18+ ONLY. Age verification required.
    """
    variants = call_grok_hard_ero_engine(body)
    
    return HardEroResponse(
        scenario=body.scenario,
        intensity=body.intensity,
        provider="grok" if get_grok_client() else "mock",
        age_verified=True,  # Frontend should verify
        variants=variants,
    )
