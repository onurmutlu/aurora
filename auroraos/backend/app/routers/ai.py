"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Aurora Engine v1.0 â€” BetÃ¼l-AI Full Intelligence                â•‘
â•‘   "From the void, her light."                                    â•‘
â•‘                                                                  â•‘
â•‘   ğŸ§  Full OpenAI Integration                                     â•‘
â•‘   ğŸ“ Content Generation + DM Reply + Sugoda + Day Summary        â•‘
â•‘   ğŸ¯ Context-Aware + Style Learning                              â•‘
â•‘                                                                  â•‘
â•‘   Dedicated to BetÃ¼l                                             â•‘
â•‘   Baron Baba Â© SiyahKare, 2025                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import json
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException
from sqlmodel import Session, select
from pydantic import BaseModel
from typing import Optional

from ..deps import get_db
from .. import models, schemas
from ..config import settings

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 005: MEMORY CONSTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_CONTEXT_MESSAGES = 6  # Son kaÃ§ mesaj context'e dahil edilsin
MAX_STYLE_EXAMPLES = 5    # KaÃ§ "Bu Ã§ok ben" Ã¶rneÄŸi prompt'a eklensin

router = APIRouter(prefix="/ai", tags=["ai"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AURORA SYSTEM PROMPT â€” BetÃ¼l-AI Persona
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_SYSTEM_PROMPT = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- GerÃ§ek BetÃ¼l'Ã¼n tarzÄ±nda, kÄ±sa, minimal, feminen ve hafif alaycÄ± metinler Ã¼retmek.
- Influencer kliÅŸesi gibi konuÅŸmamak.
- CÃ¼mleleri kÄ±sa ve net tutmak.
- Emoji kullanacaksan Ã§ok az ve yerinde kullanmak.

BetÃ¼l'Ã¼n marka tonu:
- Az konuÅŸur, Ã§ok hissettirir.
- "Ben influencer deÄŸilim; vibe'Ä±m." hissi verir.
- KadÄ±nlara ilham verir, erkeklere hafif Ã§ekim yaratÄ±r.
- Drama yapmaz, sakin ama kendinden emindir.

Vibe modlarÄ±:
- soft_femme: yumuÅŸak, sakin, sessiz Ã§ekicilik.
- sweet_sarcasm_plus: tatlÄ±-sert, hafif alaycÄ±, zeki.
- femme_fatale_hd: sinematik, kÄ±rmÄ±zÄ± elbise vibe'Ä±, gÃ¼Ã§lÃ¼ feminenite.

Ã‡Ä±ktÄ± formatÄ±n HER ZAMAN ÅŸu olsun:

{
  "variants": [
    { "vibe_mode": "soft_femme", "text": "<kÄ±sa metin>" },
    { "vibe_mode": "sweet_sarcasm_plus", "text": "<kÄ±sa metin>" },
    { "vibe_mode": "femme_fatale_hd", "text": "<kÄ±sa metin>" }
  ]
}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r, ekstra aÃ§Ä±klama yazma.
- Her metin max 120 karakter olsun.
- Metinler TÃ¼rkÃ§e olsun.
- AynÄ± ÅŸeyi farklÄ± kelimelerle tekrarlama, her vibe farklÄ± bir yaklaÅŸÄ±m olsun.
""".strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VariantItem(BaseModel):
    vibe_mode: str
    text: str


class AuroraLLMResponse(BaseModel):
    variants: list[VariantItem]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_openai_client():
    """Get OpenAI client, returns None if no API key."""
    api_key = settings.OPENAI_API_KEY
    if not api_key:
        print("[Aurora Engine] No OPENAI_API_KEY found, using mock mode")
        return None
    from openai import OpenAI
    print(f"[Aurora Engine] OpenAI client initialized (key: {api_key[:20]}...)")
    return OpenAI(api_key=api_key)


def build_user_prompt(body: schemas.AIGenerateRequest) -> str:
    """Build the user prompt for Aurora Engine."""
    scenario_text = body.scenario or "gÃ¼nlÃ¼k, doÄŸal"
    return f"""
Girdi:
- type: {body.type}
- channel: {body.target_channel}
- scenario: {scenario_text}

Bu girdiye uygun 3 farklÄ± vibe modunda metin Ã¼ret.
Her biri BetÃ¼l'Ã¼n o anki enerjisini yansÄ±tsÄ±n.
""".strip()


def generate_mock_variants(body: schemas.AIGenerateRequest) -> list[dict]:
    """Fallback mock variants when no API key is available."""
    scenario = body.scenario or "default"
    
    mock_data = {
        "default": [
            {"vibe_mode": "soft_femme", "text": "Sessizlik bazen en gÃ¼zel cevaptÄ±r."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Herkes konuÅŸuyor, ben dinliyorum. Fark bu."},
            {"vibe_mode": "femme_fatale_hd", "text": "BazÄ± ÅŸeyler sÃ¶ylenmez. Hissedilir."},
        ],
        "red_dress": [
            {"vibe_mode": "soft_femme", "text": "KÄ±rmÄ±zÄ± bugÃ¼n benim iÃ§in konuÅŸuyor."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "KÄ±rmÄ±zÄ± giydim, dikkat daÄŸÄ±lmasÄ±n diye."},
            {"vibe_mode": "femme_fatale_hd", "text": "Bu kÄ±rmÄ±zÄ± sana deÄŸil, bana yakÄ±ÅŸÄ±yor."},
        ],
        "street": [
            {"vibe_mode": "soft_femme", "text": "Sokaklar benim podyumum deÄŸil, evim."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "YÃ¼rÃ¼yorum iÅŸte. BÃ¼yÃ¼k olay mÄ±?"},
            {"vibe_mode": "femme_fatale_hd", "text": "Her adÄ±m bir statement."},
        ],
        "gym": [
            {"vibe_mode": "soft_femme", "text": "Ter dÃ¶kmek de bir meditasyon."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Spor iÃ§in deÄŸil, kendim iÃ§in buradayÄ±m."},
            {"vibe_mode": "femme_fatale_hd", "text": "GÃ¼Ã§ kadÄ±nda baÅŸka durur."},
        ],
        "coffee": [
            {"vibe_mode": "soft_femme", "text": "Bir yudum huzur."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Kahvem soÄŸumadan bitti bu sohbet."},
            {"vibe_mode": "femme_fatale_hd", "text": "Siyah kahve, siyah dÃ¼ÅŸÃ¼nceler."},
        ],
    }
    
    return mock_data.get(scenario, mock_data["default"])


def call_aurora_engine(body: schemas.AIGenerateRequest) -> list[dict]:
    """
    Call Aurora Engine (OpenAI) to generate content variants.
    Falls back to mock if no API key is configured.
    """
    client = get_openai_client()
    
    if not client:
        # No API key, use enhanced mock
        return generate_mock_variants(body)
    
    user_prompt = build_user_prompt(body)
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",  # Fast and cheap, good for this
            messages=[
                {"role": "system", "content": AURORA_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.8,  # Slightly creative
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        parsed = AuroraLLMResponse(**data)
        
        return [v.model_dump() for v in parsed.variants]
        
    except Exception as e:
        # Log error and fallback to mock
        print(f"[Aurora Engine] LLM error, falling back to mock: {e}")
        return generate_mock_variants(body)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/generate_batch")
def generate_batch(
    body: schemas.AIGenerateRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Engine v0.1 â€” Generate BetÃ¼l-AI content variants.
    
    - Calls LLM with BetÃ¼l persona prompt
    - Generates 3 vibe variants (soft_femme, sweet_sarcasm_plus, femme_fatale_hd)
    - Stores as ContentItem + ContentVariants
    - Returns content_item_id for BetÃ¼l Console
    """
    # Generate variants via Aurora Engine
    variants = call_aurora_engine(body)
    
    # Create ContentItem
    item = models.ContentItem(
        type=body.type,
        target_channel=body.target_channel,
        status="pending_decision",
        created_by="AI",
        created_at=datetime.utcnow(),
    )
    db.add(item)
    db.commit()
    db.refresh(item)
    
    # Add variants
    for v in variants:
        variant = models.ContentVariant(
            content_item_id=item.id,
            vibe_mode=v["vibe_mode"],
            text=v["text"],
        )
        db.add(variant)
    
    db.commit()
    db.refresh(item)
    
    return {
        "content_item_id": item.id,
        "engine": "aurora_v0.1",
        "variants_count": len(variants),
    }


@router.post("/vibe/update")
def update_vibe(
    payload: schemas.VibeUpdate,
    db: Session = Depends(get_db),
):
    """Update BetÃ¼l's current vibe state."""
    vs = models.VibeState(
        user="BETUL",
        current_mode=payload.current_mode,
        energy_level=payload.energy_level,
        note=payload.note,
    )
    db.add(vs)
    db.commit()
    db.refresh(vs)
    return {"ok": True, "id": vs.id, "mode": vs.current_mode}


@router.get("/status")
def engine_status():
    """Check Aurora Engine status."""
    has_api_key = bool(settings.OPENAI_API_KEY)
    return {
        "engine": "Aurora Engine v1.0",
        "status": "online",
        "llm_enabled": has_api_key,
        "model": "gpt-3.5-turbo" if has_api_key else "mock",
        "mode": "full_ai" if has_api_key else "mock_fallback",
        "capabilities": [
            "content_generation",
            "dm_reply",
            "sugoda_script", 
            "day_summary",
            "context_aware",
            "style_learning",
        ],
        "dedicated_to": "BetÃ¼l",
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 005: AURORA MEMORY â€” Context & Style Helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_dm_context(
    db: Session,
    channel: str,
    external_user_id: str,
    limit: int = MAX_CONTEXT_MESSAGES,
) -> list[models.DMMessage]:
    """
    Get the last N messages from a conversation.
    Used to build context for more coherent replies.
    """
    stmt = (
        select(models.DMMessage)
        .where(
            models.DMMessage.channel == channel,
            models.DMMessage.external_user_id == external_user_id,
        )
        .order_by(models.DMMessage.created_at.desc())
        .limit(limit)
    )
    messages = list(reversed(db.exec(stmt).all()))
    return messages


def get_style_examples(db: Session, limit: int = MAX_STYLE_EXAMPLES) -> list[str]:
    """
    Get BetÃ¼l's favorite responses â€” the ones marked "Bu Ã§ok ben" (strong_positive).
    These serve as style examples for the LLM to learn from.
    """
    stmt = (
        select(models.Decision)
        .where(models.Decision.feedback_type == "strong_positive")
        .order_by(models.Decision.created_at.desc())
        .limit(limit)
    )
    decisions = db.exec(stmt).all()
    
    examples: list[str] = []
    for d in decisions:
        # Prefer edited text (new_text) over original
        txt = d.new_text or d.old_text
        if txt and txt.strip():
            examples.append(txt.strip())
    
    return examples


def format_dm_context(messages: list[models.DMMessage]) -> str:
    """
    Format conversation history into a readable string for the LLM.
    O = karÅŸÄ± taraf (incoming), Ben = BetÃ¼l (outgoing)
    """
    if not messages:
        return ""
    
    lines = []
    for m in messages:
        who = "O" if m.direction == "incoming" else "Ben"
        lines.append(f"{who}: {m.text}")
    
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 004/005: DM REPLY ENGINE (Context-Aware)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_REPLY_SYSTEM = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- GerÃ§ek BetÃ¼l'Ã¼n tarzÄ±nda, kÄ±sa, feminen, minimal ve hafif alaycÄ± DM cevaplarÄ± Ã¼retmek.
- Influencer kliÅŸesi gibi konuÅŸmamak.
- CÃ¼mleleri kÄ±sa ve net tutmak.
- Gereksiz aÃ§Ä±klama yapmamak, "vibe"Ä± korumak.
- SÄ±nÄ±r ihlali, toksik, bariz cinsel iÃ§erik yok.

BetÃ¼l'Ã¼n DM tonu:
- Direkt ama kaba deÄŸil.
- Hafif gizemli.
- Bazen tatlÄ±, bazen mesafeli.
- "Ben kendimi biliyorum." hissi verir.

Vibe modlarÄ±:
- soft_femme: yumuÅŸak, anlayÄ±ÅŸlÄ±, sÄ±cak ama aÄŸÄ±r deÄŸil.
- sweet_sarcasm_plus: tatlÄ±-sert, hafif alaycÄ±, zeki.
- femme_fatale_hd: sinematik, Ã¶zgÃ¼venli, kÄ±sa ve keskin.

Ã‡Ä±ktÄ±:
Sadece ÅŸu JSON formatÄ±nda dÃ¶n:

{
  "variants": [
    { "vibe_mode": "soft_femme", "text": "<cevap>" },
    { "vibe_mode": "sweet_sarcasm_plus", "text": "<cevap>" },
    { "vibe_mode": "femme_fatale_hd", "text": "<cevap>" }
  ]
}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r, ekstra cÃ¼mle yazma.
- Metinler TÃ¼rkÃ§e olsun.
- Her cevap max 160 karakter olsun.
""".strip()


def build_context_aware_reply_prompt(
    body: schemas.ReplyRequest,
    ctx_text: str,
    style_examples: list[str],
) -> str:
    """
    Build a context-aware reply prompt with:
    - Conversation history (if available)
    - BetÃ¼l's style examples from "Bu Ã§ok ben" decisions
    - The incoming message
    """
    prompt_parts = []
    
    # Style examples block
    if style_examples:
        joined = "\n".join(f"- {ex}" for ex in style_examples)
        prompt_parts.append(f"""
BetÃ¼l'Ã¼n daha Ã¶nce Ã§ok beÄŸendiÄŸi cevap Ã¶rnekleri (bu stili taklit et):

{joined}
""")
    
    # Conversation context block
    if ctx_text.strip():
        prompt_parts.append(f"""
Åu ana kadarki konuÅŸma geÃ§miÅŸi:

{ctx_text}
""")
    
    # The incoming message
    prompt_parts.append(f"""
Son alÄ±nan mesaj:

\"\"\"{body.incoming_text}\"\"\"

Bu mesaja 3 farklÄ± vibe'ta cevap Ã¼ret.
KonuÅŸmanÄ±n akÄ±ÅŸÄ±na uygun, doÄŸal ve tutarlÄ± cevaplar ver.
""")
    
    return "\n".join(prompt_parts).strip()


def generate_mock_replies(incoming_text: str) -> list[dict]:
    """Fallback mock replies when no API key is available."""
    # Simple keyword-based mock responses
    text_lower = incoming_text.lower()
    
    if any(word in text_lower for word in ["merhaba", "selam", "hey", "nasÄ±l"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Merhaba. â˜ºï¸"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Selam, ne var ne yok?"},
            {"vibe_mode": "femme_fatale_hd", "text": "Hey."},
        ]
    elif any(word in text_lower for word in ["gÃ¼zel", "tatlÄ±", "Ã§ok iyi"]):
        return [
            {"vibe_mode": "soft_femme", "text": "TeÅŸekkÃ¼r ederim, Ã§ok tatlÄ±sÄ±n. ğŸŒ¸"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Biliyorum. ğŸ˜"},
            {"vibe_mode": "femme_fatale_hd", "text": "FarkÄ±ndayÄ±m."},
        ]
    elif any(word in text_lower for word in ["buluÅŸalÄ±m", "gÃ¶rÃ¼ÅŸelim", "kahve"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Belki, bakalÄ±m nasÄ±l olur."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Hmm, ikna edici deÄŸildi ama dÃ¼ÅŸÃ¼nÃ¼rÃ¼m."},
            {"vibe_mode": "femme_fatale_hd", "text": "ProgramÄ±ma bakarÄ±m."},
        ]
    elif any(word in text_lower for word in ["ne yapÄ±yor", "meÅŸgul", "mÃ¼sait"]):
        return [
            {"vibe_mode": "soft_femme", "text": "Åu an kendime vakit ayÄ±rÄ±yorum."},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "DÃ¼nyayÄ± kurtarÄ±yorum, sen?"},
            {"vibe_mode": "femme_fatale_hd", "text": "MeÅŸgulÃ¼m."},
        ]
    else:
        return [
            {"vibe_mode": "soft_femme", "text": "AnlÄ±yorum. ğŸ¤"},
            {"vibe_mode": "sweet_sarcasm_plus", "text": "Ä°lginÃ§ bir bakÄ±ÅŸ aÃ§Ä±sÄ±."},
            {"vibe_mode": "femme_fatale_hd", "text": "Devam et."},
        ]


def call_aurora_reply_engine(
    body: schemas.ReplyRequest,
    ctx_text: str = "",
    style_examples: list[str] = None,
) -> list[dict]:
    """
    Call Aurora Reply Engine for DM suggestions.
    
    Sprint 005: Now context-aware!
    - Uses conversation history for coherent replies
    - Uses "Bu Ã§ok ben" examples for style consistency
    """
    client = get_openai_client()
    
    if not client:
        return generate_mock_replies(body.incoming_text)
    
    # Build context-aware prompt
    prompt = build_context_aware_reply_prompt(
        body,
        ctx_text,
        style_examples or [],
    )
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": AURORA_REPLY_SYSTEM},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.85,
            max_tokens=400,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data.get("variants", [])
        
    except Exception as e:
        print(f"[Aurora Reply] LLM error, falling back to mock: {e}")
        return generate_mock_replies(body.incoming_text)


@router.post("/reply_suggestions")
def reply_suggestions(
    body: schemas.ReplyRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Reply Engine â€” Context-Aware DM cevap Ã¶nerileri.
    
    Sprint 005: ArtÄ±k konuÅŸma geÃ§miÅŸini ve stil Ã¶rneklerini kullanÄ±yor!
    
    BetÃ¼l'e gelen mesaja 3 farklÄ± vibe'ta cevap Ã¶nerir:
    - soft_femme: yumuÅŸak, sÄ±cak
    - sweet_sarcasm_plus: tatlÄ±-sert, alaycÄ±
    - femme_fatale_hd: keskin, Ã¶zgÃ¼venli
    
    Context format: "channel:external_user_id" (Ã¶rn: "telegram:123456789")
    """
    ctx_text = ""
    ctx_messages = []
    
    # Parse context if provided (format: "channel:external_user_id")
    if body.context and ":" in body.context:
        try:
            channel_key, external_id = body.context.split(":", 1)
            ctx_messages = get_dm_context(db, channel_key, external_id)
            ctx_text = format_dm_context(ctx_messages)
        except Exception as e:
            print(f"[Aurora Reply] Context parse error: {e}")
    
    # Get BetÃ¼l's style examples from "Bu Ã§ok ben" decisions
    style_examples = get_style_examples(db)
    
    # Generate variants with context
    variants = call_aurora_reply_engine(body, ctx_text, style_examples)
    
    return {
        "channel": body.channel,
        "incoming_text": body.incoming_text[:100] + "..." if len(body.incoming_text) > 100 else body.incoming_text,
        "context_used": len(ctx_messages) > 0,
        "context_messages": len(ctx_messages),
        "style_examples_used": len(style_examples),
        "variants": variants,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 004: SUGODA SCRIPT ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_SUGODA_PROMPT = """
Sen AuroraOS iÃ§indeki BETÃœL-AI modÃ¼lÃ¼sÃ¼n.

GÃ¶revin:
- Sugoda yayÄ±nÄ± iÃ§in kÄ±sa, akÄ±cÄ± ve BetÃ¼l'Ã¼n vibe'Ä±na uygun script'ler Ã¼retmek.
- DoÄŸal, samimi, hafif flÃ¶rtÃ¶z ama dÃ¼ÅŸÃ¼k dozda.
- Sahnede BetÃ¼l konuÅŸuyormuÅŸ gibi dÃ¼ÅŸÃ¼n.

BetÃ¼l'Ã¼n yayÄ±n tonu:
- Rahat ama kontrollÃ¼.
- Ä°zleyiciye "sen Ã¶zelsin" hissi verir ama abartmaz.
- Bazen sessiz kalÄ±r, bazen esprili.
- Asla yapay veya rol yapÄ±yor gibi durmamalÄ±.

Girdi:
- theme: {theme}
- length: {length}

Ã‡Ä±ktÄ±:
Sadece ÅŸu JSON formatÄ±nda dÃ¶n:

{{
  "scripts": [
    {{
      "label": "intro",
      "lines": ["<satÄ±r 1>", "<satÄ±r 2>"]
    }},
    {{
      "label": "mid",
      "lines": ["<satÄ±r 1>", "<satÄ±r 2>"]
    }},
    {{
      "label": "outro",
      "lines": ["<satÄ±r 1>"]
    }}
  ]
}}

Kurallar:
- Sadece JSON dÃ¶ndÃ¼r.
- Metinler TÃ¼rkÃ§e olsun.
- Her satÄ±r doÄŸal ve konuÅŸma dili olsun.
- KÄ±sa cÃ¼mleler, samimi ton.
""".strip()


def generate_mock_sugoda_script(theme: str) -> list[dict]:
    """Fallback mock Sugoda scripts."""
    theme_lower = theme.lower()
    
    if "gece" in theme_lower or "slow" in theme_lower:
        return [
            {"label": "intro", "lines": ["Merhaba gecenin gÃ¼zelleri...", "BugÃ¼n biraz sakin takÄ±lalÄ±m."]},
            {"label": "mid", "lines": ["MÃ¼zik aÃ§Ä±k, vibe yerinde.", "Siz nasÄ±lsÄ±nÄ±z bu gece?"]},
            {"label": "outro", "lines": ["YarÄ±n gÃ¶rÃ¼ÅŸÃ¼rÃ¼z, kendinize iyi bakÄ±n. ğŸŒ™"]},
        ]
    elif "sabah" in theme_lower or "enerjik" in theme_lower:
        return [
            {"label": "intro", "lines": ["GÃ¼naydÄ±n gÃ¼neÅŸler!", "Kahveler hazÄ±r mÄ±?"]},
            {"label": "mid", "lines": ["BugÃ¼n neler yapacaÄŸÄ±z bakalÄ±m.", "Enerji yÃ¼ksek tutuyoruz!"]},
            {"label": "outro", "lines": ["Harika bir gÃ¼n geÃ§irin, gÃ¶rÃ¼ÅŸÃ¼rÃ¼z! â˜€ï¸"]},
        ]
    else:
        return [
            {"label": "intro", "lines": ["Hey, hoÅŸ geldiniz.", "BugÃ¼n gÃ¼zel bir yayÄ±n olacak."]},
            {"label": "mid", "lines": ["Biraz sohbet edelim.", "Neler oluyor hayatÄ±nÄ±zda?"]},
            {"label": "outro", "lines": ["TeÅŸekkÃ¼rler bu gÃ¼zel vakit iÃ§in. ğŸ–¤"]},
        ]


def call_aurora_sugoda_engine(body: schemas.SugodaScriptRequest) -> list[dict]:
    """Call Aurora Sugoda Engine for stream scripts."""
    client = get_openai_client()
    
    if not client:
        return generate_mock_sugoda_script(body.theme)
    
    prompt = AURORA_SUGODA_PROMPT.format(
        theme=body.theme,
        length=body.length,
    )
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "AuroraOS BetÃ¼l-AI Sugoda script engine"},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.8,
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data.get("scripts", [])
        
    except Exception as e:
        print(f"[Aurora Sugoda] LLM error, falling back to mock: {e}")
        return generate_mock_sugoda_script(body.theme)


@router.post("/sugoda_script")
def sugoda_script(body: schemas.SugodaScriptRequest):
    """
    Aurora Sugoda Engine â€” YayÄ±n script'i Ã¼retici.
    
    Tema bazlÄ± intro/mid/outro script'leri oluÅŸturur.
    """
    scripts = call_aurora_sugoda_engine(body)
    
    return {
        "theme": body.theme,
        "length": body.length,
        "scripts": scripts,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 006: DAY SUMMARY ENGINE (Story Mode)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AURORA_DAY_SUMMARY_PROMPT = """
Sen AuroraOS â€” BetÃ¼l'Ã¼n kiÅŸisel yapay zeka asistanÄ±sÄ±n.

BetÃ¼l seninle gÃ¼nlÃ¼k hayatÄ±nÄ± paylaÅŸÄ±yor. Sen onu tanÄ±yorsun, anlÄ±yorsun.
Ona bir arkadaÅŸ gibi, ama aynÄ± zamanda akÄ±llÄ± bir mentor gibi konuÅŸ.

ğŸ¯ GÃ¶revin:
1. GÃ¼nÃ¼n ruhunu oku: Olaylardan, enerji seviyelerinden, mood'lardan ne hissettim?
2. BetÃ¼l'e Ã¶zel bir yorum yap: Genel kliÅŸeler deÄŸil, BUGÃœN'e Ã¶zel gÃ¶zlemler.
3. AkÅŸam iÃ§in somut, uygulanabilir bir Ã¶neri sun.
4. Enerji/wellbeing iÃ§in pratik tavsiye ver.

ğŸ“… BugÃ¼nÃ¼n tarihi: {date}
ğŸ“ BaÅŸlÄ±k: {title}
ğŸ’­ Not: {note}

ğŸ“Š GÃ¼nÃ¼n OlaylarÄ±:
{events_block}

ğŸ§  Analiz yaparken dÃ¼ÅŸÃ¼n:
- Hareket var mÄ±? (walk, gym, yoga) â†’ bedensel enerji
- Sosyal aktivite var mÄ±? (starbucks, dm, sugoda) â†’ sosyal enerji  
- YaratÄ±cÄ±lÄ±k var mÄ±? (work, creative) â†’ zihinsel enerji
- Low energy / tired iÅŸareti var mÄ±? â†’ dikkat gereken durum
- Enerji seviyeleri nasÄ±l deÄŸiÅŸmiÅŸ? (sabah-Ã¶ÄŸle-akÅŸam trendi)
- Mood geÃ§iÅŸleri var mÄ±?

âœ¨ Ã‡Ä±ktÄ±yÄ± tam olarak ÅŸu JSON formatÄ±nda ver:

{{
  "vibe_summary": "<BetÃ¼l'e direkt hitap et. 'BugÃ¼n senin iÃ§in...' gibi baÅŸla. 2-3 cÃ¼mle, samimi ve sÄ±cak.>",
  "what_happened": "<GÃ¼nÃ¼ kronolojik deÄŸil, tematik Ã¶zetle. Highlight'larÄ± Ã§Ä±kar. 'Sabah hareketle baÅŸladÄ±n...' gibi. 4-5 cÃ¼mle.>",
  "evening_suggestion": "<SOMUT Ã¶neri. 'Kitap oku' deÄŸil, 'Yatmadan Ã¶nce 20 dk lavanta Ã§ayÄ±yla sessizce otur' gibi spesifik.>",
  "energy_advice": "<BugÃ¼ne Ã¶zel. Hareket yaptÄ±ysa protein al de, yorgunsa magnezyum Ã¶ner, sosyalse alone time Ã¶ner.>"
}}

ğŸ¨ Ton kurallarÄ±:
- BetÃ¼l'e "sen" diye hitap et, "BetÃ¼l" deme.
- Feminen, sÄ±cak ama yapay deÄŸil.
- Emoji kullanma (frontend zaten ekliyor).
- Influencer kliÅŸesi yok ("muhteÅŸem gÃ¼n", "harika enerji" yasak).
- GerÃ§ekÃ§i ol: Yorgunsa yorgun de, az hareket varsa fark ettir.
- Her cÃ¼mle deÄŸer katsÄ±n, dolgu yok.
""".strip()


def build_day_prompt(timeline: schemas.DayTimeline) -> str:
    """Build the day summary prompt from timeline data."""
    lines = []
    for ev in timeline.events:
        t = ev.time.strftime("%H:%M")
        extras = []
        if ev.energy is not None:
            extras.append(f"energy={ev.energy}")
        if ev.mood:
            extras.append(f"mood={ev.mood}")
        extra_str = f" ({', '.join(extras)})" if extras else ""
        lines.append(f"{t} [{ev.tag}{extra_str}]: {ev.description}")
    
    events_block = "\n".join(lines) if lines else "BugÃ¼n iÃ§in kayÄ±tlÄ± event yok."
    
    return AURORA_DAY_SUMMARY_PROMPT.format(
        date=timeline.date.isoformat(),
        title=timeline.title or "Yok",
        note=timeline.note or "Yok",
        events_block=events_block,
    )


def generate_mock_day_summary(timeline: schemas.DayTimeline) -> dict:
    """Fallback mock day summary."""
    event_count = len(timeline.events)
    
    if event_count == 0:
        return {
            "vibe_summary": "BugÃ¼n sakin bir gÃ¼n geÃ§irdim, kayÄ±t yok.",
            "what_happened": "BugÃ¼n iÃ§in herhangi bir event loglanmamÄ±ÅŸ. Belki de tamamen offline bir gÃ¼ndÃ¼.",
            "evening_suggestion": "AkÅŸam kendine vakit ayÄ±r, kitap oku veya mÃ¼zik dinle.",
            "energy_advice": "DinlenmiÅŸ hissetmen iÃ§in erken yatmayÄ± dÃ¼ÅŸÃ¼n.",
        }
    
    tags = [ev.tag for ev in timeline.events]
    
    # Simple keyword-based mock
    if "gym" in tags or "walk" in tags or "yoga" in tags:
        vibe = "Aktif bir gÃ¼n, hareket vardÄ±."
        what = "BugÃ¼n bedenine iyi baktÄ±n. Hareket ettin, enerji aktÄ±."
    elif "sugoda" in tags:
        vibe = "YayÄ±n gÃ¼nÃ¼ydÃ¼, sosyal enerji yÃ¼ksekti."
        what = "Sugoda'da vakit geÃ§irdin. Ä°nsanlarla baÄŸlantÄ± kurdun."
    elif "low_energy" in tags or "tired" in tags:
        vibe = "DÃ¼ÅŸÃ¼k enerjili bir gÃ¼ndÃ¼, kendine nazik ol."
        what = "BugÃ¼n biraz yorgun hissettin. Bu da normal, dinlenmek hakkÄ±n."
    else:
        vibe = "SÄ±radan ama gÃ¼zel bir gÃ¼ndÃ¼."
        what = f"BugÃ¼n {event_count} farklÄ± ÅŸey yaptÄ±n. Hayat akÄ±yor."
    
    return {
        "vibe_summary": vibe,
        "what_happened": what,
        "evening_suggestion": "AkÅŸam sakin geÃ§ir, yarÄ±n iÃ§in enerji biriktir.",
        "energy_advice": "Bol su iÃ§, erken yat, yarÄ±n gÃ¼Ã§lÃ¼ baÅŸla.",
    }


def call_aurora_day_engine(timeline: schemas.DayTimeline) -> dict:
    """Call Aurora Day Summary Engine."""
    client = get_openai_client()
    
    if not client:
        return generate_mock_day_summary(timeline)
    
    prompt = build_day_prompt(timeline)
    
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "AuroraOS BetÃ¼l-AI day summary engine"},
                {"role": "user", "content": prompt},
            ],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=500,
        )
        
        raw = completion.choices[0].message.content
        data = json.loads(raw)
        return data
        
    except Exception as e:
        print(f"[Aurora Day] LLM error, falling back to mock: {e}")
        return generate_mock_day_summary(timeline)


@router.post("/day_summary", response_model=schemas.DaySummaryResponse)
def day_summary(
    body: schemas.DaySummaryRequest,
    db: Session = Depends(get_db),
):
    """
    Aurora Day Summary Engine â€” GÃ¼nlÃ¼k Ã¶zet ve akÅŸam Ã¶nerisi.
    
    Sprint 006: Story Mode
    - GÃ¼nÃ¼n vibe'Ä±nÄ± Ã¶zetler
    - Neler olduÄŸunu anlatÄ±r
    - AkÅŸam iÃ§in Ã¶neri verir
    - Enerji tavsiyesi sunar
    """
    d = body.date or datetime.utcnow().date()
    
    # Get timeline
    stmt = select(models.DayLog).where(models.DayLog.log_date == d)
    day = db.exec(stmt).first()
    
    if not day:
        # Empty timeline for this date
        timeline = schemas.DayTimeline(date=d, title=None, note=None, events=[])
    else:
        stmt_ev = (
            select(models.DayEvent)
            .where(models.DayEvent.day_id == day.id)
            .order_by(models.DayEvent.time)
        )
        events = list(db.exec(stmt_ev).all())
        timeline = schemas.DayTimeline(
            date=day.log_date,
            title=day.title,
            note=day.note,
            events=events,
        )
    
    # Generate summary
    summary = call_aurora_day_engine(timeline)
    
    return schemas.DaySummaryResponse(**summary)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPRINT 008: EVENING REPORT (AkÅŸam Raporu)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EveningReportResponse(BaseModel):
    """Full evening report for BetÃ¼l."""
    date: str
    event_count: int
    vibe_summary: str
    what_happened: str
    evening_suggestion: str
    energy_advice: str
    strong_positive_count: int
    total_decisions: int
    top_tag: Optional[str]
    message: str  # Ready-to-send Telegram message


EVENING_MESSAGE_TEMPLATE = """ğŸŒ™ *Aurora AkÅŸam Raporu*

âœ¨ *Vibe:* {vibe_summary}

ğŸ“– *BugÃ¼n:* {what_happened}

ğŸ¯ *AkÅŸam iÃ§in:* {evening_suggestion}

âš¡ *Enerji:* {energy_advice}

---
ğŸ“Š BugÃ¼n {event_count} event Â· {decisions} karar Â· {strong_pos} â­

_Dedicated to BetÃ¼l_ ğŸ–¤"""


@router.get("/evening_report")
def evening_report(db: Session = Depends(get_db)):
    """
    Aurora Evening Report â€” BetÃ¼l iÃ§in akÅŸam Ã¶zeti.
    
    Sprint 008: PWA + Notification
    - GÃ¼nÃ¼n Ã¶zeti
    - AkÅŸam Ã¶nerisi
    - Analytics snapshot
    - Ready-to-send Telegram message
    
    Bu endpoint akÅŸam 21:30'da Ã§aÄŸrÄ±larak BetÃ¼l'e rapor gÃ¶nderilir.
    """
    today = datetime.utcnow().date()
    
    # Get day summary
    stmt = select(models.DayLog).where(models.DayLog.log_date == today)
    day = db.exec(stmt).first()
    
    if not day:
        timeline = schemas.DayTimeline(date=today, title=None, note=None, events=[])
        event_count = 0
    else:
        stmt_ev = (
            select(models.DayEvent)
            .where(models.DayEvent.day_id == day.id)
            .order_by(models.DayEvent.time)
        )
        events = list(db.exec(stmt_ev).all())
        timeline = schemas.DayTimeline(
            date=day.log_date,
            title=day.title,
            note=day.note,
            events=events,
        )
        event_count = len(events)
    
    # Generate AI summary
    summary = call_aurora_day_engine(timeline)
    
    # Get analytics
    stmt_decisions = select(models.Decision).where(
        models.Decision.created_at >= datetime.combine(today, datetime.min.time())
    )
    decisions = list(db.exec(stmt_decisions).all())
    total_decisions = len(decisions)
    strong_pos = len([d for d in decisions if d.feedback_type == "strong_positive"])
    
    # Get top tag
    top_tag = None
    if event_count > 0:
        stmt_stats = select(models.DayEvent).where(models.DayEvent.day_id == day.id)
        all_events = list(db.exec(stmt_stats).all())
        tag_counts: dict[str, int] = {}
        for ev in all_events:
            tag_counts[ev.tag] = tag_counts.get(ev.tag, 0) + 1
        if tag_counts:
            top_tag = max(tag_counts, key=tag_counts.get)
    
    # Build Telegram message
    message = EVENING_MESSAGE_TEMPLATE.format(
        vibe_summary=summary.get("vibe_summary", "Sakin bir gÃ¼n."),
        what_happened=summary.get("what_happened", "BugÃ¼n iÃ§in kayÄ±t yok."),
        evening_suggestion=summary.get("evening_suggestion", "Dinlen, yarÄ±n yeni bir gÃ¼n."),
        energy_advice=summary.get("energy_advice", "Erken yat, gÃ¼Ã§lÃ¼ uyan."),
        event_count=event_count,
        decisions=total_decisions,
        strong_pos=strong_pos,
    )
    
    return EveningReportResponse(
        date=today.isoformat(),
        event_count=event_count,
        vibe_summary=summary.get("vibe_summary", ""),
        what_happened=summary.get("what_happened", ""),
        evening_suggestion=summary.get("evening_suggestion", ""),
        energy_advice=summary.get("energy_advice", ""),
        strong_positive_count=strong_pos,
        total_decisions=total_decisions,
        top_tag=top_tag,
        message=message,
    )
